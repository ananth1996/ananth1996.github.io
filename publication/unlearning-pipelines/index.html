<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Ananth Mahadevan"><meta name=description content="Machine unlearning is the task of updating machine learning (ML) models after a subset of the training data they were trained on is deleted. Methods for the task are desired to combine effectiveness and efficiency (i.e., they should effectively &amp;ldquo;unlearn&amp;rdquo; deleted data, but in a way that does not require excessive computational effort (e.g., a full retraining) for a small amount of deletions). Such a combination is typically achieved by tolerating some amount of approximation in the unlearning. In addition, laws and regulations in the spirit of &amp;ldquo;the right to be forgotten&amp;rdquo; have given rise to requirements for certifiability (i.e., the ability to demonstrate that the deleted data has indeed been unlearned by the ML model). In this paper, we present an experimental study of the three state-of-the-art approximate unlearning methods for logistic regression and demonstrate the trade-offs between efficiency, effectiveness and certifiability offered by each method. In implementing this study, we extend some of the existing works and describe a common unlearning pipeline to compare and evaluate the unlearning methods on six real-world datasets and a variety of settings. We provide insights into the effect of the quantity and distribution of the deleted data on ML models and the performance of each unlearning method in different settings. We also propose a practical online strategy to determine when the accumulated error from approximate unlearning is large enough to warrant a full retraining of the ML model."><link rel=alternate hreflang=en-us href=http://www.ananthmahadevan.com/publication/unlearning-pipelines/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.3495fc6150afdd177f1d04fbba9f5e2c.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_180x180_fill_lanczos_center_3.png><link rel=canonical href=http://www.ananthmahadevan.com/publication/unlearning-pipelines/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@Mahadea96"><meta property="twitter:creator" content="@Mahadea96"><meta property="og:site_name" content="Ananth Mahadevan"><meta property="og:url" content="http://www.ananthmahadevan.com/publication/unlearning-pipelines/"><meta property="og:title" content="Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study | Ananth Mahadevan"><meta property="og:description" content="Machine unlearning is the task of updating machine learning (ML) models after a subset of the training data they were trained on is deleted. Methods for the task are desired to combine effectiveness and efficiency (i.e., they should effectively &amp;ldquo;unlearn&amp;rdquo; deleted data, but in a way that does not require excessive computational effort (e.g., a full retraining) for a small amount of deletions). Such a combination is typically achieved by tolerating some amount of approximation in the unlearning. In addition, laws and regulations in the spirit of &amp;ldquo;the right to be forgotten&amp;rdquo; have given rise to requirements for certifiability (i.e., the ability to demonstrate that the deleted data has indeed been unlearned by the ML model). In this paper, we present an experimental study of the three state-of-the-art approximate unlearning methods for logistic regression and demonstrate the trade-offs between efficiency, effectiveness and certifiability offered by each method. In implementing this study, we extend some of the existing works and describe a common unlearning pipeline to compare and evaluate the unlearning methods on six real-world datasets and a variety of settings. We provide insights into the effect of the quantity and distribution of the deleted data on ML models and the performance of each unlearning method in different settings. We also propose a practical online strategy to determine when the accumulated error from approximate unlearning is large enough to warrant a full retraining of the ML model."><meta property="og:image" content="http://www.ananthmahadevan.com/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="http://www.ananthmahadevan.com/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-09-12T17:21:17+00:00"><meta property="article:modified_time" content="2022-09-12T20:20:35+03:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.ananthmahadevan.com/publication/unlearning-pipelines/"},"headline":"Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study","datePublished":"2022-09-12T17:21:17Z","dateModified":"2022-09-12T20:20:35+03:00","author":{"@type":"Person","name":"Ananth Mahadevan"},"publisher":{"@type":"Organization","name":"Ananth Mahadevan","logo":{"@type":"ImageObject","url":"http://www.ananthmahadevan.com/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_192x192_fill_lanczos_center_3.png"}},"description":"Machine unlearning is the task of updating machine learning (ML) models after a subset of the training data they were trained on is deleted. Methods for the task are desired to combine effectiveness and efficiency (i.e., they should effectively \u0026ldquo;unlearn\u0026rdquo; deleted data, but in a way that does not require excessive computational effort (e.g., a full retraining) for a small amount of deletions). Such a combination is typically achieved by tolerating some amount of approximation in the unlearning. In addition, laws and regulations in the spirit of \u0026ldquo;the right to be forgotten\u0026rdquo; have given rise to requirements for certifiability (i.e., the ability to demonstrate that the deleted data has indeed been unlearned by the ML model). In this paper, we present an experimental study of the three state-of-the-art approximate unlearning methods for logistic regression and demonstrate the trade-offs between efficiency, effectiveness and certifiability offered by each method. In implementing this study, we extend some of the existing works and describe a common unlearning pipeline to compare and evaluate the unlearning methods on six real-world datasets and a variety of settings. We provide insights into the effect of the quantity and distribution of the deleted data on ML models and the performance of each unlearning method in different settings. We also propose a practical online strategy to determine when the accumulated error from approximate unlearning is large enough to warrant a full retraining of the ML model."}</script><title>Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study | Ananth Mahadevan</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=61a48f5a3e68474aeda389d45f81172a><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Ananth Mahadevan</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Ananth Mahadevan</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class="nav-link active" href=/publication/><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/uploads/Resume.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study</h1><div class=article-metadata><div><span class=author-highlighted>Ananth Mahadevan</span>, <span>Michael Mathioudakis</span></div><span class=article-date>June, 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=/publication/unlearning-pipelines/unlearning-pipelines.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/unlearning-pipelines/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://version.helsinki.fi/mahadeva/unlearning-experiments target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header" href=https://doi.org/10.3390/make4030028 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2106.15093 target=_blank rel=noopener>Arvix</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Machine unlearning is the task of updating machine learning (ML) models after a subset of the training data they were trained on is deleted. Methods for the task are desired to combine effectiveness and efficiency (i.e., they should effectively “unlearn” deleted data, but in a way that does not require excessive computational effort (e.g., a full retraining) for a small amount of deletions). Such a combination is typically achieved by tolerating some amount of approximation in the unlearning. In addition, laws and regulations in the spirit of “the right to be forgotten” have given rise to requirements for certifiability (i.e., the ability to demonstrate that the deleted data has indeed been unlearned by the ML model). In this paper, we present an experimental study of the three state-of-the-art approximate unlearning methods for logistic regression and demonstrate the trade-offs between efficiency, effectiveness and certifiability offered by each method. In implementing this study, we extend some of the existing works and describe a common unlearning pipeline to compare and evaluate the unlearning methods on six real-world datasets and a variety of settings. We provide insights into the effect of the quantity and distribution of the deleted data on ML models and the performance of each unlearning method in different settings. We also propose a practical online strategy to determine when the accumulated error from approximate unlearning is large enough to warrant a full retraining of the ML model.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#2>Journal article</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9"><em>Machine Learning and Knowledge Extraction</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=http://www.ananthmahadevan.com/publication/unlearning-pipelines/&amp;text=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=http://www.ananthmahadevan.com/publication/unlearning-pipelines/&amp;t=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study&amp;body=http://www.ananthmahadevan.com/publication/unlearning-pipelines/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=http://www.ananthmahadevan.com/publication/unlearning-pipelines/&amp;title=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study%20http://www.ananthmahadevan.com/publication/unlearning-pipelines/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=http://www.ananthmahadevan.com/publication/unlearning-pipelines/&amp;title=Certifiable%20Unlearning%20Pipelines%20for%20Logistic%20Regression:%20An%20Experimental%20Study" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=http://www.ananthmahadevan.com><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hue3ffad70859443acdc3141988d65886b_206625_270x270_fill_q75_lanczos_center.jpeg alt="Ananth Mahadevan"></a><div class=media-body><h5 class=card-title><a href=http://www.ananthmahadevan.com>Ananth Mahadevan</a></h5><h6 class=card-subtitle>Machine Learning PhD Student</h6><p class=card-text>My research interests include systems for Machine Learning and network science.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:ananth.mahadevan@helsinki.fi><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?hl=en&amp;user=z-LNyRsAAAAJ" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/ananth1996 target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/ananth-mahadevan-75411651/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/uploads/Resume.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Ananth Mahadevan. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.2621b0c3d1130efc0b22bddd510b0848.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.85290d887e7fcdd400ccb3ffb9bcd3e3.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>