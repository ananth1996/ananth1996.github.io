<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning | Ananth Mahadevan</title><link>http://www.ananthmahadevan.com/tag/machine-learning/</link><atom:link href="http://www.ananthmahadevan.com/tag/machine-learning/index.xml" rel="self" type="application/rss+xml"/><description>Machine Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>http://www.ananthmahadevan.com/media/icon_hu83a2933a4b16038170999f1c101a10c2_11917_512x512_fill_lanczos_center_3.png</url><title>Machine Learning</title><link>http://www.ananthmahadevan.com/tag/machine-learning/</link></image><item><title>HPC-HD: High Performance Computing for the Detection and Analysis of Historical Discourses</title><link>http://www.ananthmahadevan.com/project/hpc-hd/</link><pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate><guid>http://www.ananthmahadevan.com/project/hpc-hd/</guid><description>&lt;p>This project will use HPC to detect discourses from large historical corpora of the eighteenth century (e.g., books, pamphlets, newspapers), and study the interconnections and evolution of the detected discourses. The approach is to analyze historical corpora in a nuanced, thorough fashion: nuanced, because we analyze the available corpora at various levels of conceptual granularity, starting from the raw documents as first elements, and then progressively discovering intermediate linguistic elements (keywords, topics, genres) and higher-level notions (concepts such as “the economy” or “the state” and discourses about them); and thorough, in the sense that the analysis is performed jointly over the entire corpora (billions of words, comprising a large fraction of all existing literature from the period). This approach contrasts traditional historical scholarship, which often uses a single element as a starting point (e.g., a passage attributed to a single well-known historical figure) and then aims to generalize from it, typically using a limited number of documents as corroborating sources. In addition, our approach also contrasts modern historical scholarship, which uses “big data” but performs the analysis at a very aggregate level. Compared to previous scholarship, our approach has the potential to discover unknown and richer insights from historical corpora that traditional approaches have missed.&lt;/p>
&lt;p>We expect that such discoveries will occur not through a one-shot computation, but as the outcome of historian-guided exploration in iterative computational workflows. The use of HPC is instrumental in building such workflows for the study of historical corpora. Specifically, HPC resources will be crucial for: the storage, processing, and management of large data volumes; building and deploying large and complex NLP models that are robust to noise and biases in the data; and, finally, providing the guiding historian with explanations for the results and efficiently adapting the existing workflow to the instruction of the historian. In developing and implementing its reusable workflows, the project will use the analysis of economic discourse in the eighteenth century as a case study.&lt;/p></description></item></channel></rss>